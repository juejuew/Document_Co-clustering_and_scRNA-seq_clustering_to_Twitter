{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages need for data pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from numpy import savetxt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# For evaluztion (NMI)\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "# Running time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('Twitter_mani.csv')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = df.text.str.lower().values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('@', '', sent) for sent in data]\n",
    "\n",
    "# Remove hashtages\n",
    "data = [re.sub('#', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove url\n",
    "data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "\n",
    "def preProcessingFcn(tweet, \n",
    "    removeNumbers=True, removePunctuation=True):\n",
    "    \n",
    "    tweet = re.sub(r\"\\\\n\", \" \", tweet)\n",
    "    tweet = re.sub(r\"&amp\", \" \", tweet)\n",
    "\n",
    "    if removeNumbers==True:\n",
    "        tweet=  ''.join(i for i in tweet if not i.isdigit())\n",
    "    if removePunctuation==True:\n",
    "        tweet = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", tweet)\n",
    "  \n",
    "    return tweet\n",
    "\n",
    "data = [preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['leave', 'it', 'all', 'on', 'the', 'field', 'umichfootball', 'best', 'rivalry', 'in', 'college', 'football', 'goblue', 'beatosu'], ['there', 'no', 'time', 'to', 'look', 'backwards', 'only', 'ahead', 'hyped', 'to', 'watch', 'the', 'umichfootball', 'squad', 'ball', 'out', 'goblue', 'beatosu']]\n",
      "12780\n"
     ]
    }
   ],
   "source": [
    "# simple_preprocess() tokenies the text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:3])\n",
    "print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_unigrams = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tweet, stem=True):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    if stem==True:\n",
    "        tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "data_stemming = [stemming(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_stemming1 = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming1.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141434\n",
      "10241\n",
      "1321\n",
      "12439\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(merged))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Identify words that appears at least 20 times\n",
    "c = Counter(merged)\n",
    "a = list(Counter({k: c for k, c in c.items() if c >= 20}).keys())\n",
    "\n",
    "# Select words that appears at least 20 times\n",
    "for i,value in enumerate(data_stemming):\n",
    "    data_stemming[i] = [i for i in value if i in a] \n",
    "    \n",
    "# Check\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Get the index of the doc that are deleted\n",
    "empty_idx = []\n",
    "\n",
    "for i, value in enumerate(data_stemming):\n",
    "    if any(value) == False:\n",
    "        empty_idx.append(i)\n",
    "len(empty_idx)\n",
    "\n",
    "# Delete empty elements\n",
    "data_stemming2 = list(filter(None, data_stemming))\n",
    "print(len(data_stemming2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming2)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming2\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA number of clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"storm\" + 0.019*\"week\" + 0.017*\"tropic\" + 0.017*\"across\" + 0.014*\"thunderstorm\" + 0.013*\"flood\" + 0.012*\"heat\" + 0.012*\"weekend\" + 0.011*\"state\" + 0.011*\"weather\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.027*\"health\" + 0.021*\"care\" + 0.018*\"plan\" + 0.018*\"ppfa\" + 0.017*\"peopl\" + 0.012*\"parenthood\" + 0.012*\"get\" + 0.012*\"need\" + 0.011*\"abort\" + 0.011*\"women\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.055*\"goblu\" + 0.031*\"month\" + 0.022*\"michigan\" + 0.016*\"one\" + 0.016*\"year\" + 0.016*\"first\" + 0.013*\"night\" + 0.011*\"game\" + 0.011*\"day\" + 0.011*\"meteor\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.082*\"vegan\" + 0.046*\"wave\" + 0.028*\"recip\" + 0.023*\"summer\" + 0.021*\"made\" + 0.015*\"free\" + 0.014*\"bake\" + 0.013*\"make\" + 0.013*\"easi\" + 0.012*\"tofu\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = 4, random_state = 88)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12439"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(df.drop(df.index[empty_idx]))\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new[\"tokenize\"] = data_stemming2\n",
    "\n",
    "True_Label = list(df_new[\"user_id_new\"])\n",
    "len(True_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the topic distribution for the given document, #doc in bow format\n",
    "df_doc_topic = pd.DataFrame(columns = ['Doc_ID', 'Dominant_Topic', 'Prob_Per_Topic', 'Keywords', 'Doc_Text'])\n",
    "for i in range(len(corpus)):\n",
    "  a = lda_model.get_document_topics(corpus[i])\n",
    "  a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "  topic_wp = lda_model.show_topic(a[0][0]) #show top 10 words byb default\n",
    "  keywords = [w for (w,p) in topic_wp]\n",
    "  row = [i, a[0][0], a[0][1], keywords, df_new['text'][i]]\n",
    "  df_doc_topic.loc[i] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7399748575610817,\n",
       " 0.8302899343978929,\n",
       " 0.7733913510880616,\n",
       " 0.6779834913561048,\n",
       " 0.5783425578979882,\n",
       " 0.4803218768670751,\n",
       " 0.5389942479563443,\n",
       " 0.5201222066956117,\n",
       " 0.4759204956956093,\n",
       " 0.5146658441288589,\n",
       " 0.5308044079310624,\n",
       " 0.5621249884131062,\n",
       " 0.4650568948561562,\n",
       " 0.45314799469794576,\n",
       " 0.30815857482653414,\n",
       " 0.2587460707810341,\n",
       " 0.21285696651342484,\n",
       " 0.22652266493368278]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NMI\n",
    "LDA_NMI = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])\n",
    "    NMI = normalized_mutual_info_score(True_Label, pred_LDA)\n",
    "    LDA_NMI.append(NMI)\n",
    "\n",
    "LDA_NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purity\n",
    "Correct_target = pd.DataFrame(df[[\"screen_name\", \"user_id_new\"]])\n",
    "Correct_target = Correct_target.drop(Correct_target.index[empty_idx]).reset_index(drop=True)\n",
    "df_doc_topic = df_doc_topic.reset_index(drop=True)\n",
    "\n",
    "LDA_purity = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    df_compare = pd.concat([df_doc_topic, Correct_target.reindex(df_doc_topic.index)], axis=1)\n",
    "    nominator = df_compare.groupby([\"Dominant_Topic\", \"user_id_new\"], as_index=False)['screen_name'].count().sort_values('screen_name', ascending=False).drop_duplicates('Dominant_Topic')[\"screen_name\"].sum()\n",
    "    purity = nominator/len(True_Label)\n",
    "    LDA_purity.append(purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7435485167617976,\n",
       " 0.9490312726103385,\n",
       " 0.9314253557359916,\n",
       " 0.8899429214567087,\n",
       " 0.8379290939786156,\n",
       " 0.8288447624407107,\n",
       " 0.8500683334673205,\n",
       " 0.8453251869121312,\n",
       " 0.8198408232173005,\n",
       " 0.8685585658011095,\n",
       " 0.8773213280810355,\n",
       " 0.867191896454699,\n",
       " 0.8141329688881743,\n",
       " 0.8153388536055953,\n",
       " 0.7158131682611142,\n",
       " 0.679556234423989,\n",
       " 0.6367875231127904,\n",
       " 0.6382345847736957]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_purity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The running time can also be measured simultaneously with the evaluation (when computing NMI and Purity)\n",
    "LDA_running = []\n",
    "\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    start = timer()\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    end = timer()\n",
    "    time = end-start\n",
    "    \n",
    "    LDA_running.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.541619300000093,\n",
       " 22.577719400000205,\n",
       " 20.95631869999943,\n",
       " 21.261016599999493,\n",
       " 21.80628969999998,\n",
       " 22.98331120000057,\n",
       " 21.160482700000102,\n",
       " 22.066481699999713,\n",
       " 21.694230300000527,\n",
       " 19.81751599999916,\n",
       " 18.270815500000026,\n",
       " 18.087509399999362,\n",
       " 17.463955399999577,\n",
       " 17.076652200000353,\n",
       " 17.320095299999593,\n",
       " 17.406181000000288,\n",
       " 17.385939900000267,\n",
       " 16.72735289999946]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ITCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jueju\\.conda\\envs\\Python3.7\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io as io\n",
    "from sklearn.metrics import (adjusted_rand_score as ari,\n",
    "                             normalized_mutual_info_score as nmi)\n",
    "from coclust.coclustering import CoclustInfo\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from coclust.evaluation.external import accuracy\n",
    "from coclust.visualization import plot_delta_kl, plot_convergence\n",
    "from sklearn.utils import check_random_state, check_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "sparse_matrix = pd.read_csv('doc_word_matrix_stemmingf.csv', header=None)\n",
    "sparse = sparse_matrix.values\n",
    "sparse.shape\n",
    "\n",
    "b = np.array(sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7043098381694087,\n",
       " 0.6790122721336654,\n",
       " 0.7857821059446214,\n",
       " 0.7431394332895197,\n",
       " 0.7111411982219402,\n",
       " 0.7058730392336214,\n",
       " 0.6835868815976629,\n",
       " 0.6478897226216574,\n",
       " 0.6376926514554008,\n",
       " 0.6306894019118364,\n",
       " 0.6160624803396079,\n",
       " 0.6015075030104314,\n",
       " 0.588349457219723,\n",
       " 0.5996390481582456,\n",
       " 0.5877944928673002,\n",
       " 0.5783823089706712,\n",
       " 0.5696578756948969,\n",
       " 0.5597403009596387]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NMI\n",
    "ITCC_NMI = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    model = CoclustInfo(n_row_clusters=num_cluster, n_col_clusters=num_cluster,\n",
    "                    n_init=4, random_state=88)\n",
    "    model.fit(b)\n",
    "    n = list(model.row_labels_)\n",
    "    NMI = normalized_mutual_info_score(True_Label, n)\n",
    "    ITCC_NMI.append(NMI)\n",
    "    \n",
    "ITCC_NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7382274812847138,\n",
       " 0.8385253159462288,\n",
       " 0.9469532319085567,\n",
       " 0.9314980278515657,\n",
       " 0.9560492634629316,\n",
       " 0.9579811639700555,\n",
       " 0.9434919101666264,\n",
       " 0.9381791837720357,\n",
       " 0.9426869516219915,\n",
       " 0.959510585204862,\n",
       " 0.9500120743781695,\n",
       " 0.9567737261531031,\n",
       " 0.9561297593173952,\n",
       " 0.957900668115592,\n",
       " 0.9564517427352491,\n",
       " 0.9574176929888111,\n",
       " 0.9574981888432745,\n",
       " 0.9530709168477823]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purity\n",
    "# Purity\n",
    "ITCC_purity = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    model = CoclustInfo(n_row_clusters=num_cluster, n_col_clusters=num_cluster,\n",
    "                    n_init=4, random_state=88)\n",
    "    model.fit(b)\n",
    "    n = pd.DataFrame(list(model.row_labels_), columns = [\"Dominant_Topic\"])\n",
    "    df_compare = pd.concat([n, Correct_target.reindex(df_doc_topic.index)], axis=1)\n",
    "    nominator = df_compare.groupby([\"Dominant_Topic\", \"user_id_new\"], as_index=False)['screen_name'].count().sort_values('screen_name', ascending=False).drop_duplicates('Dominant_Topic')[\"screen_name\"].sum()\n",
    "    purity = nominator/12423\n",
    "    ITCC_purity.append(purity)\n",
    "    \n",
    "ITCC_purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.832472800000687,\n",
       " 7.42417949999799,\n",
       " 7.225789999996778,\n",
       " 7.350030799992965,\n",
       " 8.15186180001183,\n",
       " 7.34670000000915,\n",
       " 7.6426400000054855,\n",
       " 7.444389099997352,\n",
       " 7.949637400000938,\n",
       " 7.740193699995871,\n",
       " 8.105400100001134,\n",
       " 7.899508100003004,\n",
       " 8.160881699994206,\n",
       " 7.857870299994829,\n",
       " 8.25968859999557,\n",
       " 7.968454999994719,\n",
       " 8.350172899998142,\n",
       " 8.143195399999968]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITCC_Running = []\n",
    "\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    start = timer()\n",
    "    model = CoclustInfo(n_row_clusters=num_cluster, n_col_clusters=num_cluster,\n",
    "                    n_init=4, random_state=88)\n",
    "    model.fit(b)\n",
    "    end = timer()\n",
    "    time = end-start\n",
    "    \n",
    "    ITCC_Running.append(time)\n",
    "\n",
    "ITCC_Running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
