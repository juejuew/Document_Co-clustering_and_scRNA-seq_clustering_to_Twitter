{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages need for data pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv(\"f_jobs_tweets_sampled_three_month.csv\", encoding= 'unicode_escape')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>sn</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-08-01 10:25:36</td>\n",
       "      <td>Now Hiring:  Storage Architect II http://bit.l...</td>\n",
       "      <td>ChicagoJobAds</td>\n",
       "      <td>2009-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-08-01 22:57:06</td>\n",
       "      <td>\"The Steve Jobs method\" discussion on Hacker N...</td>\n",
       "      <td>hnshah</td>\n",
       "      <td>2009-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-01 23:27:08</td>\n",
       "      <td>AZ Jobs | Taco Bell Restaurant General Manager...</td>\n",
       "      <td>ZuluJobsAZ</td>\n",
       "      <td>2009-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-08-01 09:55:12</td>\n",
       "      <td>TN Jobs | SLP Travel Job in Knoxville Area, TN...</td>\n",
       "      <td>ZuluJobsTN</td>\n",
       "      <td>2009-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-08-01 05:58:39</td>\n",
       "      <td>NJ Jobs | New Jersey Travel or Perm job- OT at...</td>\n",
       "      <td>ZuluJobsNJ</td>\n",
       "      <td>2009-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>2009-11-01 02:15:14</td>\n",
       "      <td>these guys have to wake up. make him work alre...</td>\n",
       "      <td>yankee32879</td>\n",
       "      <td>2009-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27896</th>\n",
       "      <td>2009-11-01 03:04:26</td>\n",
       "      <td>Therapy Jobs at HCR! Physical Therapist / PT -...</td>\n",
       "      <td>lydsterj2w</td>\n",
       "      <td>2009-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27897</th>\n",
       "      <td>2009-11-01 00:21:24</td>\n",
       "      <td>hospitality jobs http://bit.ly/3XvUT1</td>\n",
       "      <td>Ur_WebInfoNews</td>\n",
       "      <td>2009-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27898</th>\n",
       "      <td>2009-11-01 03:26:41</td>\n",
       "      <td>Obama Tempers Economic News With Caution On Jo...</td>\n",
       "      <td>suzanne_newton</td>\n",
       "      <td>2009-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27899</th>\n",
       "      <td>2009-11-01 03:21:23</td>\n",
       "      <td>EXCITING, getting ready for my 1st job test =D...</td>\n",
       "      <td>NadaAbdulrazak</td>\n",
       "      <td>2009-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27900 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                                               text  \\\n",
       "0      2009-08-01 10:25:36  Now Hiring:  Storage Architect II http://bit.l...   \n",
       "1      2009-08-01 22:57:06  \"The Steve Jobs method\" discussion on Hacker N...   \n",
       "2      2009-08-01 23:27:08  AZ Jobs | Taco Bell Restaurant General Manager...   \n",
       "3      2009-08-01 09:55:12  TN Jobs | SLP Travel Job in Knoxville Area, TN...   \n",
       "4      2009-08-01 05:58:39  NJ Jobs | New Jersey Travel or Perm job- OT at...   \n",
       "...                    ...                                                ...   \n",
       "27895  2009-11-01 02:15:14  these guys have to wake up. make him work alre...   \n",
       "27896  2009-11-01 03:04:26  Therapy Jobs at HCR! Physical Therapist / PT -...   \n",
       "27897  2009-11-01 00:21:24              hospitality jobs http://bit.ly/3XvUT1   \n",
       "27898  2009-11-01 03:26:41  Obama Tempers Economic News With Caution On Jo...   \n",
       "27899  2009-11-01 03:21:23  EXCITING, getting ready for my 1st job test =D...   \n",
       "\n",
       "                   sn        date  \n",
       "0       ChicagoJobAds  2009-08-01  \n",
       "1              hnshah  2009-08-01  \n",
       "2          ZuluJobsAZ  2009-08-01  \n",
       "3          ZuluJobsTN  2009-08-01  \n",
       "4          ZuluJobsNJ  2009-08-01  \n",
       "...               ...         ...  \n",
       "27895     yankee32879  2009-11-01  \n",
       "27896      lydsterj2w  2009-11-01  \n",
       "27897  Ur_WebInfoNews  2009-11-01  \n",
       "27898  suzanne_newton  2009-11-01  \n",
       "27899  NadaAbdulrazak  2009-11-01  \n",
       "\n",
       "[27900 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = df.text.str.lower().values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('@', '', sent) for sent in data]\n",
    "\n",
    "# Remove hashtages\n",
    "data = [re.sub('#', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove url\n",
    "data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "\n",
    "# Remove space\n",
    "data = [re.sub(r\"\\\\n\", \" \", sent) for sent in data]\n",
    "\n",
    "# Remove meaningless symbol\n",
    "data = [re.sub(r\"&amp\", \" \", sent) for sent in data]\n",
    "\n",
    "# Remove number\n",
    "data =  [''.join(i for i in sent if not i.isdigit()) for sent in data]\n",
    "    \n",
    "# Remove punctuation\n",
    "data =  [re.sub(r\"[,.;@#?!&$]+\\ *\", ' ', sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['now', 'hiring', 'storage', 'architect', 'ii', 'jobs'], ['the', 'steve', 'jobs', 'method', 'discussion', 'on', 'hacker', 'news', 'via', 'ericries'], ['az', 'jobs', 'taco', 'bell', 'restaurant', 'general', 'manager', 'at', 'taco', 'bell', 'peoria', 'az', 'job', 'hiring', 'azjobs']]\n",
      "27900\n"
     ]
    }
   ],
   "source": [
    "# simple_preprocess() tokenies the text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:3])\n",
    "print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_unigrams = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tweet, stem=True):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    if stem==True:\n",
    "        tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "data_stemming = [stemming(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stemming1 = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming1.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289174\n",
      "21565\n",
      "1829\n",
      "27898\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(merged))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Identify words that appears at least 20 times\n",
    "c = Counter(merged)\n",
    "a = list(Counter({k: c for k, c in c.items() if c >= 20}).keys())\n",
    "\n",
    "# Select words that appears at least 20 times\n",
    "for i,value in enumerate(data_stemming):\n",
    "    data_stemming[i] = [j for j in value if j in a] \n",
    "    \n",
    "# Check\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Get the index of the doc that are deleted\n",
    "empty_idx = []\n",
    "\n",
    "for i, value in enumerate(data_stemming):\n",
    "    if any(value) == False:\n",
    "        empty_idx.append(i)\n",
    "len(empty_idx)\n",
    "\n",
    "# Delete empty elements\n",
    "data_stemming2 = list(filter(None, data_stemming))\n",
    "print(len(data_stemming2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming2)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming2\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27898, 1829)\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a_s = gensim.matutils.corpus2dense(corpus, num_terms = 1829)\n",
    "\n",
    "# Create Doc-word matrix\n",
    "b_s = a_s.T.astype(np.float64)\n",
    "print(b_s.shape)\n",
    "print(b_s)\n",
    "\n",
    "#savetxt('jobs_doc_word_matrix_stemmingf.csv', b_s, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Document index\n",
    "selected_idex = [x for x in list(df.index) if x not in empty_idx]\n",
    "\n",
    "# Obtain remaining terms\n",
    "words = [] \n",
    "for i,j in enumerate(id2word):\n",
    "    a = id2word[i]\n",
    "    words.append(a)\n",
    "\n",
    "# Create a dataframe\n",
    "b_ss = pd.DataFrame(b_s, columns=words, index=selected_idex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architect</th>\n",
       "      <th>hire</th>\n",
       "      <th>ii</th>\n",
       "      <th>job</th>\n",
       "      <th>discuss</th>\n",
       "      <th>news</th>\n",
       "      <th>steve</th>\n",
       "      <th>via</th>\n",
       "      <th>az</th>\n",
       "      <th>azjob</th>\n",
       "      <th>...</th>\n",
       "      <th>advis</th>\n",
       "      <th>hazmat</th>\n",
       "      <th>caution</th>\n",
       "      <th>pogu</th>\n",
       "      <th>airway</th>\n",
       "      <th>effort</th>\n",
       "      <th>oct</th>\n",
       "      <th>iwow</th>\n",
       "      <th>persist</th>\n",
       "      <th>overst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27896</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27897</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27898</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27898 rows Ã— 1829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       architect  hire   ii  job  discuss  news  steve  via   az  azjob  ...  \\\n",
       "0            1.0   1.0  1.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "1            0.0   0.0  0.0  1.0      1.0   1.0    1.0  1.0  0.0    0.0  ...   \n",
       "2            0.0   1.0  0.0  2.0      0.0   0.0    0.0  0.0  2.0    1.0  ...   \n",
       "3            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "4            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "...          ...   ...  ...  ...      ...   ...    ...  ...  ...    ...  ...   \n",
       "27895        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "27896        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "27897        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "27898        0.0   0.0  0.0  1.0      0.0   2.0    0.0  0.0  0.0    0.0  ...   \n",
       "27899        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
       "\n",
       "       advis  hazmat  caution  pogu  airway  effort  oct  iwow  persist  \\\n",
       "0        0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "1        0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "2        0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "3        0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "4        0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "...      ...     ...      ...   ...     ...     ...  ...   ...      ...   \n",
       "27895    0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "27896    0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "27897    0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "27898    0.0     0.0      1.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "27899    0.0     0.0      0.0   0.0     0.0     0.0  0.0   0.0      0.0   \n",
       "\n",
       "       overst  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "27895     0.0  \n",
       "27896     0.0  \n",
       "27897     0.0  \n",
       "27898     0.0  \n",
       "27899     0.0  \n",
       "\n",
       "[27898 rows x 1829 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_ss.to_csv(\"f_jobs_doc_word_matrix_stemmingf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>sn</th>\n",
       "      <th>date</th>\n",
       "      <th>tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-08-01 10:25:36</td>\n",
       "      <td>Now Hiring:  Storage Architect II http://bit.l...</td>\n",
       "      <td>ChicagoJobAds</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>[hire, architect, ii, job]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-08-01 22:57:06</td>\n",
       "      <td>\"The Steve Jobs method\" discussion on Hacker N...</td>\n",
       "      <td>hnshah</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>[steve, job, discuss, news, via]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-01 23:27:08</td>\n",
       "      <td>AZ Jobs | Taco Bell Restaurant General Manager...</td>\n",
       "      <td>ZuluJobsAZ</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>[az, job, taco, bell, restaur, gener, manag, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-08-01 09:55:12</td>\n",
       "      <td>TN Jobs | SLP Travel Job in Knoxville Area, TN...</td>\n",
       "      <td>ZuluJobsTN</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>[tn, job, slp, travel, job, knoxvil, area, tn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-08-01 05:58:39</td>\n",
       "      <td>NJ Jobs | New Jersey Travel or Perm job- OT at...</td>\n",
       "      <td>ZuluJobsNJ</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>[nj, job, new, jersey, travel, perm, job, ot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>2009-11-01 02:15:14</td>\n",
       "      <td>these guys have to wake up. make him work alre...</td>\n",
       "      <td>yankee32879</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>[guy, make, work, alreadi, job]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27896</th>\n",
       "      <td>2009-11-01 03:04:26</td>\n",
       "      <td>Therapy Jobs at HCR! Physical Therapist / PT -...</td>\n",
       "      <td>lydsterj2w</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>[therapi, job, hcr, physic, therapist, pt, prn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27897</th>\n",
       "      <td>2009-11-01 00:21:24</td>\n",
       "      <td>hospitality jobs http://bit.ly/3XvUT1</td>\n",
       "      <td>Ur_WebInfoNews</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>[hospit, job]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27898</th>\n",
       "      <td>2009-11-01 03:26:41</td>\n",
       "      <td>Obama Tempers Economic News With Caution On Jo...</td>\n",
       "      <td>suzanne_newton</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>[obama, temper, econom, news, caution, job, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27899</th>\n",
       "      <td>2009-11-01 03:21:23</td>\n",
       "      <td>EXCITING, getting ready for my 1st job test =D...</td>\n",
       "      <td>NadaAbdulrazak</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>[excit, get, readi, st, job, test, one, dream,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27898 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                                               text  \\\n",
       "0      2009-08-01 10:25:36  Now Hiring:  Storage Architect II http://bit.l...   \n",
       "1      2009-08-01 22:57:06  \"The Steve Jobs method\" discussion on Hacker N...   \n",
       "2      2009-08-01 23:27:08  AZ Jobs | Taco Bell Restaurant General Manager...   \n",
       "3      2009-08-01 09:55:12  TN Jobs | SLP Travel Job in Knoxville Area, TN...   \n",
       "4      2009-08-01 05:58:39  NJ Jobs | New Jersey Travel or Perm job- OT at...   \n",
       "...                    ...                                                ...   \n",
       "27895  2009-11-01 02:15:14  these guys have to wake up. make him work alre...   \n",
       "27896  2009-11-01 03:04:26  Therapy Jobs at HCR! Physical Therapist / PT -...   \n",
       "27897  2009-11-01 00:21:24              hospitality jobs http://bit.ly/3XvUT1   \n",
       "27898  2009-11-01 03:26:41  Obama Tempers Economic News With Caution On Jo...   \n",
       "27899  2009-11-01 03:21:23  EXCITING, getting ready for my 1st job test =D...   \n",
       "\n",
       "                   sn        date  \\\n",
       "0       ChicagoJobAds  2009-08-01   \n",
       "1              hnshah  2009-08-01   \n",
       "2          ZuluJobsAZ  2009-08-01   \n",
       "3          ZuluJobsTN  2009-08-01   \n",
       "4          ZuluJobsNJ  2009-08-01   \n",
       "...               ...         ...   \n",
       "27895     yankee32879  2009-11-01   \n",
       "27896      lydsterj2w  2009-11-01   \n",
       "27897  Ur_WebInfoNews  2009-11-01   \n",
       "27898  suzanne_newton  2009-11-01   \n",
       "27899  NadaAbdulrazak  2009-11-01   \n",
       "\n",
       "                                                tokenize  \n",
       "0                             [hire, architect, ii, job]  \n",
       "1                       [steve, job, discuss, news, via]  \n",
       "2      [az, job, taco, bell, restaur, gener, manag, t...  \n",
       "3      [tn, job, slp, travel, job, knoxvil, area, tn,...  \n",
       "4      [nj, job, new, jersey, travel, perm, job, ot, ...  \n",
       "...                                                  ...  \n",
       "27895                    [guy, make, work, alreadi, job]  \n",
       "27896  [therapi, job, hcr, physic, therapist, pt, prn...  \n",
       "27897                                      [hospit, job]  \n",
       "27898  [obama, temper, econom, news, caution, job, mo...  \n",
       "27899  [excit, get, readi, st, job, test, one, dream,...  \n",
       "\n",
       "[27898 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_metadata = df.loc[selected_idex,:]\n",
    "doc_metadata[\"tokenize\"] =  data_stemming2\n",
    "doc_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_metadata.to_csv(\"f_jobs_stemming_meta_doc.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
