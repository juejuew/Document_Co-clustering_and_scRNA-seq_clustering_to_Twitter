{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages need for data pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from numpy import savetxt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# For evaluztion (NMI)\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "# Running time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('Twitter_mani.csv')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = df.text.str.lower().values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('@', '', sent) for sent in data]\n",
    "\n",
    "# Remove hashtages\n",
    "data = [re.sub('#', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove url\n",
    "data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "\n",
    "def preProcessingFcn(tweet, \n",
    "    removeNumbers=True, removePunctuation=True):\n",
    "    \n",
    "    tweet = re.sub(r\"\\\\n\", \" \", tweet)\n",
    "    tweet = re.sub(r\"&amp\", \" \", tweet)\n",
    "\n",
    "    if removeNumbers==True:\n",
    "        tweet=  ''.join(i for i in tweet if not i.isdigit())\n",
    "    if removePunctuation==True:\n",
    "        tweet = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", tweet)\n",
    "  \n",
    "    return tweet\n",
    "\n",
    "data = [preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['leave', 'it', 'all', 'on', 'the', 'field', 'umichfootball', 'best', 'rivalry', 'in', 'college', 'football', 'goblue', 'beatosu'], ['there', 'no', 'time', 'to', 'look', 'backwards', 'only', 'ahead', 'hyped', 'to', 'watch', 'the', 'umichfootball', 'squad', 'ball', 'out', 'goblue', 'beatosu']]\n",
      "12780\n"
     ]
    }
   ],
   "source": [
    "# simple_preprocess() tokenies the text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:3])\n",
    "print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_unigrams = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tweet, stem=True):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    if stem==True:\n",
    "        tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "data_stemming = [stemming(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_stemming1 = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming1.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141434\n",
      "10241\n",
      "1321\n",
      "12439\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(merged))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Identify words that appears at least 20 times\n",
    "c = Counter(merged)\n",
    "a = list(Counter({k: c for k, c in c.items() if c >= 20}).keys())\n",
    "\n",
    "# Select words that appears at least 20 times\n",
    "for i,value in enumerate(data_stemming):\n",
    "    data_stemming[i] = [i for i in value if i in a] \n",
    "    \n",
    "# Check\n",
    "merged = list(itertools.chain.from_iterable(data_stemming))\n",
    "print(len(set(merged)))\n",
    "\n",
    "# Get the index of the doc that are deleted\n",
    "empty_idx = []\n",
    "\n",
    "for i, value in enumerate(data_stemming):\n",
    "    if any(value) == False:\n",
    "        empty_idx.append(i)\n",
    "len(empty_idx)\n",
    "\n",
    "# Delete empty elements\n",
    "data_stemming2 = list(filter(None, data_stemming))\n",
    "print(len(data_stemming2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming2)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming2\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA number of clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"storm\" + 0.019*\"week\" + 0.017*\"tropic\" + 0.017*\"across\" + 0.014*\"thunderstorm\" + 0.013*\"flood\" + 0.012*\"heat\" + 0.012*\"weekend\" + 0.011*\"state\" + 0.011*\"weather\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.027*\"health\" + 0.021*\"care\" + 0.018*\"plan\" + 0.018*\"ppfa\" + 0.017*\"peopl\" + 0.012*\"parenthood\" + 0.012*\"get\" + 0.012*\"need\" + 0.011*\"abort\" + 0.011*\"women\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.055*\"goblu\" + 0.031*\"month\" + 0.022*\"michigan\" + 0.016*\"one\" + 0.016*\"year\" + 0.016*\"first\" + 0.013*\"night\" + 0.011*\"game\" + 0.011*\"day\" + 0.011*\"meteor\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.082*\"vegan\" + 0.046*\"wave\" + 0.028*\"recip\" + 0.023*\"summer\" + 0.021*\"made\" + 0.015*\"free\" + 0.014*\"bake\" + 0.013*\"make\" + 0.013*\"easi\" + 0.012*\"tofu\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = 4, random_state = 88)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12439"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(df.drop(df.index[empty_idx]))\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new[\"tokenize\"] = data_stemming2\n",
    "\n",
    "True_Label = list(df_new[\"user_id_new\"])\n",
    "len(True_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the topic distribution for the given document, #doc in bow format\n",
    "df_doc_topic = pd.DataFrame(columns = ['Doc_ID', 'Dominant_Topic', 'Prob_Per_Topic', 'Keywords', 'Doc_Text'])\n",
    "for i in range(len(corpus)):\n",
    "  a = lda_model.get_document_topics(corpus[i])\n",
    "  a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "  topic_wp = lda_model.show_topic(a[0][0]) #show top 10 words byb default\n",
    "  keywords = [w for (w,p) in topic_wp]\n",
    "  row = [i, a[0][0], a[0][1], keywords, df_new['text'][i]]\n",
    "  df_doc_topic.loc[i] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7399748575610817,\n",
       " 0.8302899343978929,\n",
       " 0.7733913510880616,\n",
       " 0.6779834913561048,\n",
       " 0.5783425578979882,\n",
       " 0.4803218768670751,\n",
       " 0.5389942479563443,\n",
       " 0.5201222066956117,\n",
       " 0.4759204956956093,\n",
       " 0.5146658441288589,\n",
       " 0.5308044079310624,\n",
       " 0.5621249884131062,\n",
       " 0.4650568948561562,\n",
       " 0.45314799469794576,\n",
       " 0.30815857482653414,\n",
       " 0.2587460707810341,\n",
       " 0.21285696651342484,\n",
       " 0.22652266493368278]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NMI\n",
    "LDA_NMI = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])\n",
    "    NMI = normalized_mutual_info_score(True_Label, pred_LDA)\n",
    "    LDA_NMI.append(NMI)\n",
    "\n",
    "LDA_NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purity\n",
    "Correct_target = pd.DataFrame(df[[\"screen_name\", \"user_id_new\"]])\n",
    "Correct_target = Correct_target.drop(Correct_target.index[empty_idx]).reset_index(drop=True)\n",
    "df_doc_topic = df_doc_topic.reset_index(drop=True)\n",
    "\n",
    "LDA_purity = []\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    df_compare = pd.concat([df_doc_topic, Correct_target.reindex(df_doc_topic.index)], axis=1)\n",
    "    nominator = df_compare.groupby([\"Dominant_Topic\", \"user_id_new\"], as_index=False)['screen_name'].count().sort_values('screen_name', ascending=False).drop_duplicates('Dominant_Topic')[\"screen_name\"].sum()\n",
    "    purity = nominator/len(True_Label)\n",
    "    LDA_purity.append(purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7435485167617976,\n",
       " 0.9490312726103385,\n",
       " 0.9314253557359916,\n",
       " 0.8899429214567087,\n",
       " 0.8379290939786156,\n",
       " 0.8288447624407107,\n",
       " 0.8500683334673205,\n",
       " 0.8453251869121312,\n",
       " 0.8198408232173005,\n",
       " 0.8685585658011095,\n",
       " 0.8773213280810355,\n",
       " 0.867191896454699,\n",
       " 0.8141329688881743,\n",
       " 0.8153388536055953,\n",
       " 0.7158131682611142,\n",
       " 0.679556234423989,\n",
       " 0.6367875231127904,\n",
       " 0.6382345847736957]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_purity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The running time can also be measured simultaneously with the evaluation (when computing NMI and Purity)\n",
    "LDA_running = []\n",
    "\n",
    "for i in range(0,18):\n",
    "    num_cluster = i+3\n",
    "    start = timer()\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 88)\n",
    "    end = timer()\n",
    "    time = end-start\n",
    "    \n",
    "    LDA_running.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.541619300000093,\n",
       " 22.577719400000205,\n",
       " 20.95631869999943,\n",
       " 21.261016599999493,\n",
       " 21.80628969999998,\n",
       " 22.98331120000057,\n",
       " 21.160482700000102,\n",
       " 22.066481699999713,\n",
       " 21.694230300000527,\n",
       " 19.81751599999916,\n",
       " 18.270815500000026,\n",
       " 18.087509399999362,\n",
       " 17.463955399999577,\n",
       " 17.076652200000353,\n",
       " 17.320095299999593,\n",
       " 17.406181000000288,\n",
       " 17.385939900000267,\n",
       " 16.72735289999946]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
